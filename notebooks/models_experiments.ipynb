{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b366de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading preprcoessed data\n",
      "Data has been loaded successfully. Vocab: 10225, SeqLen: 20\n",
      "\n",
      " Starting Experiments: 9 Combinations\n",
      "⏩ Skipping RNN + Adam (Already Done)\n",
      "⏩ Skipping RNN + RMSprop (Already Done)\n",
      "⏩ Skipping RNN + SGD (Already Done)\n",
      "⏩ Skipping LSTM + Adam (Already Done)\n",
      "⏩ Skipping LSTM + RMSprop (Already Done)\n",
      "⏩ Skipping LSTM + SGD (Already Done)\n",
      "TRAINING BASELINE: Transformer (2 Layers) + Adam\n",
      "Epoch 1/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 146ms/step - accuracy: 0.0389 - loss: 7.1355 - val_accuracy: 0.0382 - val_loss: 6.9674\n",
      "Epoch 2/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 163ms/step - accuracy: 0.0399 - loss: 6.7553 - val_accuracy: 0.0382 - val_loss: 7.0253\n",
      "Epoch 3/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 169ms/step - accuracy: 0.0399 - loss: 6.7044 - val_accuracy: 0.0382 - val_loss: 7.0313\n",
      "Epoch 4/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 153ms/step - accuracy: 0.0397 - loss: 6.6102 - val_accuracy: 0.0389 - val_loss: 6.9873\n",
      "Epoch 5/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 153ms/step - accuracy: 0.0400 - loss: 6.5112 - val_accuracy: 0.0415 - val_loss: 6.9772\n",
      "Epoch 6/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 152ms/step - accuracy: 0.0452 - loss: 6.4482 - val_accuracy: 0.0480 - val_loss: 6.9828\n",
      " Perplexity: 1077.93\n",
      "TRAINING BASELINE: Transformer (2 Layers) + RMSprop\n",
      "Epoch 1/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 160ms/step - accuracy: 0.0375 - loss: 7.1958 - val_accuracy: 0.0382 - val_loss: 6.9696\n",
      "Epoch 2/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 159ms/step - accuracy: 0.0397 - loss: 6.9657 - val_accuracy: 0.0382 - val_loss: 6.9632\n",
      "Epoch 3/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 159ms/step - accuracy: 0.0400 - loss: 6.9862 - val_accuracy: 0.0382 - val_loss: 6.9552\n",
      "Epoch 4/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 159ms/step - accuracy: 0.0400 - loss: 6.9931 - val_accuracy: 0.0382 - val_loss: 6.9675\n",
      "Epoch 5/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 153ms/step - accuracy: 0.0401 - loss: 6.9856 - val_accuracy: 0.0382 - val_loss: 6.9561\n",
      "Epoch 6/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 157ms/step - accuracy: 0.0400 - loss: 6.9718 - val_accuracy: 0.0382 - val_loss: 6.9575\n",
      "Epoch 7/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 159ms/step - accuracy: 0.0395 - loss: 6.9374 - val_accuracy: 0.0394 - val_loss: 6.8615\n",
      "Epoch 8/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 159ms/step - accuracy: 0.0463 - loss: 6.7863 - val_accuracy: 0.0605 - val_loss: 6.7225\n",
      "Epoch 9/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 160ms/step - accuracy: 0.0646 - loss: 6.6415 - val_accuracy: 0.0654 - val_loss: 6.6603\n",
      "Epoch 10/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 162ms/step - accuracy: 0.0716 - loss: 6.5639 - val_accuracy: 0.0676 - val_loss: 6.6460\n",
      "Epoch 11/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 160ms/step - accuracy: 0.0754 - loss: 6.5071 - val_accuracy: 0.0678 - val_loss: 6.6132\n",
      "Epoch 12/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 162ms/step - accuracy: 0.0816 - loss: 6.4532 - val_accuracy: 0.0714 - val_loss: 6.5873\n",
      "Epoch 13/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 163ms/step - accuracy: 0.0855 - loss: 6.4097 - val_accuracy: 0.0686 - val_loss: 6.6174\n",
      "Epoch 14/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 154ms/step - accuracy: 0.0903 - loss: 6.3705 - val_accuracy: 0.0692 - val_loss: 6.6450\n",
      "Epoch 15/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 154ms/step - accuracy: 0.0926 - loss: 6.3315 - val_accuracy: 0.0697 - val_loss: 6.6445\n",
      "Epoch 16/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 153ms/step - accuracy: 0.0976 - loss: 6.2893 - val_accuracy: 0.0736 - val_loss: 6.6398\n",
      "Epoch 17/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 154ms/step - accuracy: 0.1020 - loss: 6.2457 - val_accuracy: 0.0720 - val_loss: 6.6985\n",
      " Perplexity: 811.21\n",
      "TRAINING BASELINE: Transformer (2 Layers) + SGD\n",
      "Epoch 1/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 148ms/step - accuracy: 0.0373 - loss: 8.3685 - val_accuracy: 0.0382 - val_loss: 7.3055\n",
      "Epoch 2/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 149ms/step - accuracy: 0.0400 - loss: 7.2342 - val_accuracy: 0.0382 - val_loss: 7.0615\n",
      "Epoch 3/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 150ms/step - accuracy: 0.0400 - loss: 7.0381 - val_accuracy: 0.0382 - val_loss: 6.9683\n",
      "Epoch 4/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 148ms/step - accuracy: 0.0400 - loss: 6.9548 - val_accuracy: 0.0382 - val_loss: 6.9211\n",
      "Epoch 5/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 149ms/step - accuracy: 0.0400 - loss: 6.9080 - val_accuracy: 0.0382 - val_loss: 6.8951\n",
      "Epoch 6/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 148ms/step - accuracy: 0.0400 - loss: 6.8804 - val_accuracy: 0.0382 - val_loss: 6.8797\n",
      "Epoch 7/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 148ms/step - accuracy: 0.0399 - loss: 6.8609 - val_accuracy: 0.0382 - val_loss: 6.8701\n",
      "Epoch 8/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 148ms/step - accuracy: 0.0399 - loss: 6.8486 - val_accuracy: 0.0382 - val_loss: 6.8630\n",
      "Epoch 9/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 150ms/step - accuracy: 0.0401 - loss: 6.8369 - val_accuracy: 0.0382 - val_loss: 6.8587\n",
      "Epoch 10/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 149ms/step - accuracy: 0.0400 - loss: 6.8307 - val_accuracy: 0.0382 - val_loss: 6.8553\n",
      "Epoch 11/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 149ms/step - accuracy: 0.0400 - loss: 6.8250 - val_accuracy: 0.0382 - val_loss: 6.8536\n",
      "Epoch 12/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 149ms/step - accuracy: 0.0399 - loss: 6.8198 - val_accuracy: 0.0382 - val_loss: 6.8519\n",
      "Epoch 13/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 149ms/step - accuracy: 0.0397 - loss: 6.8173 - val_accuracy: 0.0382 - val_loss: 6.8508\n",
      "Epoch 14/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 149ms/step - accuracy: 0.0399 - loss: 6.8134 - val_accuracy: 0.0382 - val_loss: 6.8502\n",
      "Epoch 15/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 150ms/step - accuracy: 0.0400 - loss: 6.8113 - val_accuracy: 0.0382 - val_loss: 6.8499\n",
      "Epoch 16/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 149ms/step - accuracy: 0.0401 - loss: 6.8079 - val_accuracy: 0.0382 - val_loss: 6.8492\n",
      "Epoch 17/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 150ms/step - accuracy: 0.0399 - loss: 6.8073 - val_accuracy: 0.0382 - val_loss: 6.8491\n",
      "Epoch 18/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 151ms/step - accuracy: 0.0399 - loss: 6.8051 - val_accuracy: 0.0382 - val_loss: 6.8490\n",
      "Epoch 19/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 151ms/step - accuracy: 0.0399 - loss: 6.8036 - val_accuracy: 0.0382 - val_loss: 6.8488\n",
      "Epoch 20/20\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 152ms/step - accuracy: 0.0400 - loss: 6.8021 - val_accuracy: 0.0382 - val_loss: 6.8492\n",
      " Perplexity: 943.13\n",
      "\n",
      "All experiments completed for baseline.\n",
      "         Model Optimizer  Accuracy    Loss  Perplexity Training Time (s)  \\\n",
      "0          RNN      Adam    0.0663  6.8536      947.32            Cached   \n",
      "1          RNN   RMSprop    0.0760  6.5013      666.04            Cached   \n",
      "2          RNN       SGD    0.0382  6.8490      942.97            Cached   \n",
      "3         LSTM      Adam    0.0707  6.8695      962.45            Cached   \n",
      "4         LSTM   RMSprop    0.0749  6.5635      708.72            Cached   \n",
      "5         LSTM       SGD    0.0382  6.8538      947.45            Cached   \n",
      "6  Transformer      Adam    0.0480  6.9828     1077.93            1010.9   \n",
      "7  Transformer   RMSprop    0.0720  6.6985      811.21           2893.43   \n",
      "8  Transformer       SGD    0.0382  6.8492      943.13           3214.28   \n",
      "\n",
      "   Layers  \n",
      "0       2  \n",
      "1       2  \n",
      "2       2  \n",
      "3       2  \n",
      "4       2  \n",
      "5       2  \n",
      "6       2  \n",
      "7       2  \n",
      "8       2  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "RNN_UNITS = 128\n",
    "TRANSFORMER_HEADS = 4       \n",
    "TRANSFORMER_FF_DIM = 512    \n",
    "TRANSFORMER_BLOCKS = 2      \n",
    "DROPOUT_RATE = 0.2          \n",
    "BATCH_SIZE = 128           \n",
    "EPOCHS = 20                 \n",
    "PATIENCE = 5                \n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../results/tables', exist_ok=True)\n",
    "os.makedirs('../results/training_logs', exist_ok=True)\n",
    "\n",
    "print(\" Loading preprcoessed data\")\n",
    "data = np.load('../data/processed/ready_data.npz')\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "\n",
    "# Loading token limits\n",
    "max_sequence_len = int(data['max_sequence_len'])\n",
    "total_words = int(data['total_words'])\n",
    "print(f\"Data has been loaded successfully. Vocab: {total_words}, SeqLen: {max_sequence_len}\")\n",
    "\n",
    "\n",
    "#  Simple RNN with 2 layers one is stacked on top of the other  \n",
    "def build_rnn(vocab_size, seq_length):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=seq_length-1),\n",
    "        SimpleRNN(RNN_UNITS, return_sequences=True, dropout=DROPOUT_RATE), \n",
    "        SimpleRNN(RNN_UNITS, return_sequences=False, dropout=DROPOUT_RATE),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ], name=\"RNN\")\n",
    "    return model\n",
    "\n",
    "#LSTM with 2 layers one is stacked on top of the other\n",
    "def build_lstm(vocab_size, seq_length):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=seq_length-1),\n",
    "        # Layer 1\n",
    "        LSTM(RNN_UNITS, return_sequences=True, dropout=DROPOUT_RATE),\n",
    "        # Layer 2\n",
    "        LSTM(RNN_UNITS, return_sequences=False, dropout=DROPOUT_RATE),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ], name=\"LSTM\")\n",
    "    return model\n",
    "\n",
    "#Transformer Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) # Multi-Head Attention for Self-Attention between words so that the model can focus on different parts of the input sequence\n",
    "        self.ffn = Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]) # Feed-Forward Network to process the output of the attention mechanism\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) # Layer Normalization to stabilize and speed up training\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) # Layer Normalization to stabilize and speed up training\n",
    "        self.dropout1 = Dropout(rate) # Dropout for regularization\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs) # Self-Attention for the input sequence\n",
    "        attn_output = self.dropout1(attn_output, training=training) # Dropout for regularization\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection and Layer Normalization\n",
    "        ffn_output = self.ffn(out1) # Feed-Forward Network\n",
    "        ffn_output = self.dropout2(ffn_output, training=training) # Dropout for regularization\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__() # Initializing the layer\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim) # Token Embedding to convert word indices to dense vectors\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim) # Position Embedding to encode the position of each word in the sequence by adding positional information to the token embeddings\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1] # Getting the maximum length of the input sequence\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1) # Creating a range of positions from 0 to maxlen\n",
    "        positions = self.pos_emb(positions) # Getting the positional embeddings\n",
    "        x = self.token_emb(x) # Getting the token embeddings\n",
    "        return x + positions # Adding token and positional embeddings\n",
    "\n",
    "# Transformer (2 Blocks) \n",
    "def build_transformer(vocab_size, seq_length):\n",
    "    inputs = Input(shape=(seq_length-1,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(seq_length-1, vocab_size, EMBEDDING_DIM)\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    # Block 1\n",
    "    transformer_block1 = TransformerBlock(EMBEDDING_DIM, TRANSFORMER_HEADS, TRANSFORMER_FF_DIM, rate=DROPOUT_RATE)\n",
    "    x = transformer_block1(x)\n",
    "    \n",
    "    # Block 2 (Stacked)\n",
    "    transformer_block2 = TransformerBlock(EMBEDDING_DIM, TRANSFORMER_HEADS, TRANSFORMER_FF_DIM, rate=DROPOUT_RATE)\n",
    "    x = transformer_block2(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x) # Global Average Pooling to reduce the dimensionality of the output by taking the average of all the tokens in the sequence and then passing it through a dropout layer to prevent overfitting\n",
    "    x = Dropout(DROPOUT_RATE)(x) #Randomly turns off neurons to prevent overfitting\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Transformer\")\n",
    "    return model\n",
    "\n",
    "\n",
    "models_to_test = [\"RNN\", \"LSTM\", \"Transformer\"]\n",
    "optimizers_to_test = [\"Adam\", \"RMSprop\", \"SGD\"]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "print(f\"\\n Starting Experiments: {len(models_to_test) * len(optimizers_to_test)} Combinations\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    for opt_name in optimizers_to_test:\n",
    "\n",
    "        log_path = f\"../results/training_logs/{model_name}_{opt_name}_log.csv\"\n",
    "        model_path = f\"../models/{model_name}_{opt_name}.keras\"\n",
    "        \n",
    "        # CHECK 1: If log exists, skip training and load results!\n",
    "        if os.path.exists(log_path) and os.path.exists(model_path):\n",
    "            print(f\"⏩ Skipping {model_name} + {opt_name} (Already Done)\")\n",
    "            \n",
    "            # Load logs to get the final metrics\n",
    "            df_log = pd.read_csv(log_path)\n",
    "            last_row = df_log.iloc[-1]\n",
    "            \n",
    "            results_list.append({\n",
    "                \"Model\": model_name, \n",
    "                \"Optimizer\": opt_name,\n",
    "                \"Accuracy\": round(last_row['val_accuracy'], 4),\n",
    "                \"Loss\": round(last_row['val_loss'], 4),\n",
    "                \"Perplexity\": round(np.exp(last_row['val_loss']), 2),\n",
    "                \"Training Time (s)\": \"Cached\", # We lost the exact time, but that's okay for now\n",
    "                \"Layers\": 2\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        print(f\"TRAINING BASELINE: {model_name} (2 Layers) + {opt_name}\")\n",
    "       \n",
    "        \n",
    "        if model_name == \"RNN\":\n",
    "            model = build_rnn(total_words, max_sequence_len)\n",
    "        elif model_name == \"LSTM\":\n",
    "            model = build_lstm(total_words, max_sequence_len)\n",
    "        elif model_name == \"Transformer\":\n",
    "            model = build_transformer(total_words, max_sequence_len)\n",
    "            \n",
    "        if opt_name == \"Adam\":\n",
    "            opt = Adam(learning_rate=0.001)\n",
    "        elif opt_name == \"RMSprop\":\n",
    "            opt = RMSprop(learning_rate=0.001)\n",
    "        elif opt_name == \"SGD\":\n",
    "            opt = SGD(learning_rate=0.01) # Baseline SGD 0.01\n",
    "            \n",
    "        #  Compile\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        \n",
    "        #  Train\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=PATIENCE, monitor='val_loss', restore_best_weights=True),\n",
    "                CSVLogger(f\"../results/training_logs/{model_name}_{opt_name}_log.csv\")\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "        \n",
    "        #  Saving Model\n",
    "        model_path = f\"../models/{model_name}_{opt_name}.keras\"\n",
    "        model.save(model_path)\n",
    "        \n",
    "        #  Recording Results\n",
    "        final_val_acc = history.history['val_accuracy'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        perplexity = np.exp(final_val_loss)\n",
    "        \n",
    "        results_list.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Optimizer\": opt_name,\n",
    "            \"Accuracy\": round(final_val_acc, 4),\n",
    "            \"Loss\": round(final_val_loss, 4),\n",
    "            \"Perplexity\": round(perplexity, 2),\n",
    "            \"Training Time (s)\": round(train_time, 2),\n",
    "            \"Layers\": 2 \n",
    "        })\n",
    "        \n",
    "        print(f\" Perplexity: {round(perplexity, 2)}\")\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv('../results/tables/optimizer_comparison_baseline.csv', index=False)\n",
    "\n",
    "print(\"\\nAll experiments completed for baseline.\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
